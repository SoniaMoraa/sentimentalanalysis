{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing\n",
    "import matplotlib.pyplot as plt #visuals\n",
    "import seaborn as sns # modelling\n",
    "\n",
    "train = pd. read_excel(r'C:\\Users\\Admin\\Desktop\\new data\\train.xlsx')\n",
    "predict = pd. read_excel(r'C:\\Users\\Admin\\Desktop\\new data\\predict.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first five rows\n",
    "display(train.head(15))\n",
    "\n",
    "# Info\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lengths\n",
    "train[\"length\"] = train[\"review\"].str.len()\n",
    "\n",
    "# Get word counts\n",
    "train[\"word_count\"] = train[\"review\"].str.split().apply(len)\n",
    "\n",
    "# Display the two columns\n",
    "display(train[[\"length\", \"word_count\"]])\n",
    "\n",
    "# Look at the distribution of length and word_count\n",
    "sns.distplot(train[\"length\"], bins=10)\n",
    "plt.title(\"Distribution of review lengths\")\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(train[\"word_count\"], bins=10)\n",
    "plt.title(\"Distribution of word counts\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print 10 bins of length column\n",
    "print(pd.cut(train['length'], 10).value_counts())\n",
    "\n",
    "# Print 10 bins of word_count column\n",
    "print(pd.cut(train['word_count'], 10).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import Snowballstemmer for stemming Emglish words\n",
    "from snowballstemmer import EnglishStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "en_stemmer = EnglishStemmer()\n",
    "\n",
    "# stop words\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "ext_stopwords = [\"ni\", \"jp\", \"@\", \"na\", \"paka\", \"kumi\", \"i\", \"nini\", \"nyinyi\", \"rada\",\n",
    "                 \"hii\", \"this\", \"coz\", \"my\", \"to\", \"me\", \"yote\", \"you\", \"jackpot\", \"is\", \"a\"]\n",
    "full_stopwords = en_stopwords+ext_stopwords\n",
    "\n",
    "def tokenize(review):\n",
    "\n",
    "    # Tokenize the review\n",
    "    tokenized = word_tokenize(review, preserve_line=True)\n",
    "\n",
    "    # Remove the stopwords\n",
    "    tokenized = [token for token in tokenized if token not in full_stopwords]\n",
    "\n",
    "    # Stemming the tokens\n",
    "    tokenized = [en_stemmer.stemWord(token) for token in tokenized]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Apply the function\n",
    "train[\"Tokenized\"] = train[\"review\"].str.lower().apply(tokenize)\n",
    "predict[\"Tokenized\"] = predict[\"review\"].str.lower().apply(tokenize)\n",
    "\n",
    "# See the result\n",
    "display(train[\"Tokenized\"].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert tokenized words from list to string\n",
    "train['tokenized_str']=[\" \".join(token) for token in train['Tokenized'].values]\n",
    "\n",
    "# Initialize a Tf-idf Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the vectorizer\n",
    "tfidf_matrix = vectorizer.fit_transform(train[\"tokenized_str\"])\n",
    "\n",
    "# Let's see what we have\n",
    "display(tfidf_matrix)\n",
    "\n",
    "# Create a DataFrame for tf-idf vectors and display the first five rows\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns= vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first five rows of the tfidf DataFrame\n",
    "display(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary tools from sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Select the features and the target\n",
    "X = train['tokenized_str']\n",
    "y = train[\"code\"]\n",
    "\n",
    "# Split the train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from textblob import Word\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Loading the dataset\n",
    "data = pd.read_csv('Finance_data.csv')\n",
    "#Pre-Processing the text \n",
    "def cleaning(df, stop_words):\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "    # Replacing the digits/numbers\n",
    "    df['sentences'] = df['sentences'].str.replace('d', '')\n",
    "    # Removing stop words\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
    "    # Lemmatization\n",
    "    df['sentences'] = df['sentences'].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))\n",
    "    return df\n",
    "stop_words = stopwords.words('english')\n",
    "data_cleaned = cleaning(data, stop_words)\n",
    "#Generating Embeddings using tokenizer\n",
    "tokenizer = Tokenizer(num_words=500, split=' ') \n",
    "tokenizer.fit_on_texts(data_cleaned['verified_reviews'].values)\n",
    "X = tokenizer.texts_to_sequences(data_cleaned['verified_reviews'].values)\n",
    "X = pad_sequences(X)\n",
    "#Model Building\n",
    "model = Sequential()\n",
    "model.add(Embedding(500, 120, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(704, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(352, activation='LeakyReLU'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "#Model Training\n",
    "model.fit(X_train, y_train, epochs = 20, batch_size=32, verbose =1)\n",
    "#Model Testing\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf vectorizer\n",
    "model_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the vectorizer with X_train\n",
    "tfidf_train = model_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Tranform the vectorizer with X_test\n",
    "tfidf_test = model_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the Bernoulli Naive Bayes classifier\n",
    "nb = BernoulliNB()\n",
    "\n",
    "# Fit the model\n",
    "nb.fit(tfidf_train, y_train)\n",
    "\n",
    "# Print the accuracy score\n",
    "best_accuracy = cross_val_score(nb, tfidf_test, y_test, cv=10, scoring='accuracy').max()\n",
    "print(\"Accuracy:\",best_accuracy)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = nb.predict(tfidf_test)\n",
    "\n",
    "# Print the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(cm)\n",
    "\n",
    "# Print the Classification Report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"\\n\\nClassification Report\\n\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized words from list to string\n",
    "predict['tokenized_str']=[\" \".join(token) for token in predict['Tokenized'].values]\n",
    "\n",
    "# Look at the test data\n",
    "display(predict.head(10))\n",
    "print(predict.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tfidf of predict data\n",
    "tfidf_final = model_vectorizer.transform(predict[\"tokenized_str\"])\n",
    "\n",
    "# Predict the labels\n",
    "y_pred_final = nb.predict(tfidf_final)\n",
    "predict[\"code\"] = y_pred_final\n",
    "print(predict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72b2382ece9768098284d92bbc69d35954e75b60d1e25897d1389c232f4796f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
